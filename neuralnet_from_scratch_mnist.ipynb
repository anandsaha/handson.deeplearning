{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom neural network to classify the mnist digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: http://neuralnetworksanddeeplearning.com/chap1.html\n",
    "\n",
    "(This is a simplified version with lot's of comments, works with Python3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learnings: Always normalize your data!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utility\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import mnist_loader\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fetch training data\n",
    "master_train_labels, master_train_images = utility.read_mnist()\n",
    "master_test_labels, master_test_images = utility.read_mnist('testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the y value into a binary array\n",
    "master_train_labels_ = []\n",
    "for l in master_train_labels:\n",
    "    a = np.zeros((10, ))\n",
    "    a[l] = 1\n",
    "    master_train_labels_.append(a)\n",
    "    \n",
    "    \n",
    "master_train_labels = np.array(master_train_labels_)\n",
    "\n",
    "master_test_labels_ = []\n",
    "for l in master_test_labels:\n",
    "    a = np.zeros((10, ))\n",
    "    a[l] = 1\n",
    "    master_test_labels_.append(a)\n",
    "    \n",
    "    \n",
    "master_test_labels = np.array(master_test_labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study the shapes\n",
    "print(master_train_labels.shape)\n",
    "print(master_train_images.shape)\n",
    "print(master_test_labels.shape)\n",
    "print(master_test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at one image\n",
    "#plt.imshow(master_train_images[2], cmap=cm.gray)\n",
    "#plt.show()\n",
    "#print(master_train_labels[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the image dimensions\n",
    "print(master_train_images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do some transformations to the master data\n",
    "# Flatten the images from 28x28 to 784\n",
    "\n",
    "master_train_images = master_train_images.reshape(master_train_images.shape[0], 784,)\n",
    "master_test_images = master_test_images.reshape(master_test_images.shape[0], 784, )\n",
    "\n",
    "print(master_train_images.shape)\n",
    "print(master_test_images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some pre-processing. First we divid by 255 to get the values between 0 and 1. Then we substract the mean to make them 0 centered.\n",
    "\n",
    "master_train_images = master_train_images/255\n",
    "master_test_images = master_test_images/255\n",
    "\"\"\"\n",
    "mean_train = np.mean(master_train_images)\n",
    "mean_test = np.mean(master_test_images)\n",
    "\n",
    "master_train_images -= mean_train\n",
    "master_test_images -= mean_test\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's split the training images into training and validation sets\n",
    "training_pct = 0.8 # and 20% validation\n",
    "training_len = master_train_images.shape[0]\n",
    "\n",
    "train_mask = np.random.rand(training_len) < training_pct\n",
    "\n",
    "train_images = master_train_images[train_mask]\n",
    "train_labels = master_train_labels[train_mask]\n",
    "\n",
    "valid_images = master_train_images[~train_mask]\n",
    "valid_labels = master_train_labels[~train_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images.shape)\n",
    "print(valid_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_images))\n",
    "print(type(train_labels))\n",
    "print(type(train_images[0]))\n",
    "print(type(train_labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's use data from NNDL book\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "train_images = []\n",
    "train_labels = []\n",
    "for item in training_data:\n",
    "    train_images.append(item[0].reshape(784, ))\n",
    "    train_labels.append(item[1].reshape(10, ))\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(num_nodes, ) for num_nodes in sizes[1:]] \n",
    "        self.weights = [np.random.randn(to, frm) for frm, to in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, activation):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            activation = sigmoid(np.dot(w, activation) + b)\n",
    "        return activation\n",
    "\n",
    "    def SGD(self, iterations, xs, ys, learn_rate, xs_test, ys_test):\n",
    "        len_test = len(xs_test)\n",
    "        len_train = len(xs)\n",
    "        mini_batch_size = 10\n",
    "        \n",
    "        indexes = [i for i in range(len_train)]\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            \"\"\"\n",
    "            random.shuffle(indexes)\n",
    "            mini_batches = [indexes[k:k+mini_batch_size] for k in range(0, len_train, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(xs[mini_batch], ys[mini_batch], learn_rate)\n",
    "            \"\"\"\n",
    "            self.update_mini_batch(xs, ys, learn_rate)\n",
    "            print(\"Iteration {0}/{1}, {2}/{3}\".format(i+1, iterations, \n",
    "                                                      self.evaluate(xs_test, ys_test), len_test))\n",
    "            \n",
    "    def update_mini_batch(self, xs, ys, learn_rate):\n",
    "        upd_biases = [np.zeros(b.shape) for b in self.biases]\n",
    "        upd_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "        length = len(xs)\n",
    "        \n",
    "        for x, y in zip(xs, ys):\n",
    "            delta_biases, delta_weights = self.backprop(x, y)\n",
    "            upd_biases = [u + d for u, d in zip(upd_biases, delta_biases)]\n",
    "            upd_weights = [u + d for u, d in zip(upd_weights, delta_weights)]\n",
    "            \n",
    "        self.biases = [b - (learn_rate/length) * u for b, u in zip(self.biases, upd_biases)]\n",
    "        self.weights = [w - (learn_rate/length) * u for w, u in zip(self.weights, upd_weights)]\n",
    "        \n",
    "    def backprop(self, x, y):\n",
    "        upd_biases = [np.zeros(b.shape) for b in self.biases]\n",
    "        upd_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        activation = np.array(x)\n",
    "        activations = []\n",
    "        zs = []\n",
    "        activations.append(activation)\n",
    "        \n",
    "        # Feed forward\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        \n",
    "        # Back prop - output error\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        \n",
    "        upd_biases[-1] = delta\n",
    "        upd_weights[-1] = np.dot(np.expand_dims(delta, axis=1), \n",
    "                                 np.expand_dims(activations[-2], axis=1).transpose())\n",
    "\n",
    "        # Back prop - backpropagate the error        \n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            upd_biases[-l] = delta\n",
    "            upd_weights[-l] = np.dot(np.expand_dims(delta, axis=1), \n",
    "                                     np.expand_dims(activations[-l-1], axis=1).transpose())\n",
    "            \n",
    "        # output\n",
    "        return upd_biases, upd_weights\n",
    "    \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        # Derivative of Mean Squared Error\n",
    "        return (output_activations-y)\n",
    "    \n",
    "    def evaluate(self, xs, ys):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in zip(xs, ys)]\n",
    "        return sum(int(x == y) for (x, y) in test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#net = Network([2, 3, 2])\n",
    "#net.backprop([1, 1], [10, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network([784, 30, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.SGD(10, train_images, train_labels, 3, valid_images, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.evaluate(valid_images, valid_labels)\n",
    "# Let's look at one image\n",
    "idx = 2240\n",
    "#plt.imshow(train_images[idx].reshape((28, 28)), cmap=cm.gray)\n",
    "#plt.show()\n",
    "print(train_labels[idx])\n",
    "net.feedforward(train_images[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(net.biases))\n",
    "print(net.biases[0].shape)\n",
    "print(len(net.weights))\n",
    "print(net.weights[0].shape)\n",
    "print(train_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.feedforward(train_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script neuralnet_from_scratch_mnist.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
